{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8589b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import robosuite as suite\n",
    "# from robosuite import load_controller_config\n",
    "# from robosuite.wrappers.gym_wrapper import GymWrapper\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "from stable_baselines3 import DDPG , SAC, PPO\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "import argparse, os, glob\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,TensorDataset,random_split,DataLoader,SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d349c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.device(cuda)\n",
      "torch.cuda.device_count():  1\n",
      "Tesla V100-SXM2-16GB\n",
      "torch.cuda.current_device() 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"torch.device(cuda)\")\n",
    "    print(\"torch.cuda.device_count(): \", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name())\n",
    "    print(\"torch.cuda.current_device()\", torch.cuda.current_device())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"torch.device(cpu)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444921b",
   "metadata": {},
   "source": [
    "# Load Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bfa62456",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: robot0_joint_pos_cos, Value.shape: (15,)\n",
      "Key: robot0_joint_pos_sin, Value.shape: (15,)\n",
      "Key: robot0_joint_vel, Value.shape: (15,)\n",
      "Key: robot0_right_eef_pos, Value.shape: (3,)\n",
      "Key: robot0_right_eef_quat, Value.shape: (4,)\n",
      "Key: robot0_right_gripper_qpos, Value.shape: (2,)\n",
      "Key: robot0_right_gripper_qvel, Value.shape: (2,)\n",
      "Key: robot0_left_eef_pos, Value.shape: (3,)\n",
      "Key: robot0_left_eef_quat, Value.shape: (4,)\n",
      "Key: robot0_left_gripper_qpos, Value.shape: (2,)\n",
      "Key: robot0_left_gripper_qvel, Value.shape: (2,)\n",
      "Key: tube008_initial_pos, Value.shape: (3,)\n",
      "Key: tube008_pos, Value.shape: (3,)\n",
      "Key: tube008_quat, Value.shape: (4,)\n",
      "Key: pipette004_initial_pos, Value.shape: (3,)\n",
      "Key: pipette004_pos, Value.shape: (3,)\n",
      "Key: pipette004_quat, Value.shape: (4,)\n",
      "Key: target_pos, Value.shape: (3,)\n",
      "Key: target_quat, Value.shape: (4,)\n",
      "Key: g1_to_target_pos, Value.shape: (3,)\n",
      "Key: g1_to_target_quat, Value.shape: ()\n",
      "Key: g0_to_target_pos, Value.shape: (3,)\n",
      "Key: g0_to_target_quat, Value.shape: ()\n",
      "Key: robot0_proprio-state, Value.shape: (67,)\n",
      "Key: object-state, Value.shape: (35,)\n"
     ]
    }
   ],
   "source": [
    "controller_config = load_controller_config(default_controller=\"JOINT_POSITION\")\n",
    "env = suite.make(\n",
    "    \"MaholoLaboratory_eefR_Move2Pipette\",\n",
    "    \"Maholo\",\n",
    "    controller_configs=controller_config,\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=False,\n",
    "    use_camera_obs=False,\n",
    "    control_freq=50,\n",
    "    render_camera=\"frontview\",\n",
    "    render_gpu_device_id=0,\n",
    "    horizon=500,\n",
    "    initialization_noise=None,\n",
    ")\n",
    "for key,value in env.reset().items():\n",
    "    print(f\"Key: {key}, Value.shape: {value.shape}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9b9e3cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeFeature GYM Wrapper obs.shape: (103,)\n"
     ]
    }
   ],
   "source": [
    "env = GymWrapper(env)\n",
    "env = TimeFeatureWrapper(env)\n",
    "print(f\"TimeFeature GYM Wrapper obs.shape: {env.reset().shape}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e554cedc",
   "metadata": {},
   "source": [
    "# Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7055ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = env.reset().shape[0]\n",
    "output_size = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a5754",
   "metadata": {},
   "source": [
    "## 1. Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b803be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)\n",
    "src = torch.rand((10, 32, 512))\n",
    "tgt = torch.rand((20, 32, 512))\n",
    "out = transformer_model(src, tgt)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2ddfcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17])\n"
     ]
    }
   ],
   "source": [
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_layers):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        \n",
    "        self.embedding_src = nn.Linear(input_size, d_model)\n",
    "        self.embedding_tgt = nn.Linear(output_size, d_model)\n",
    "        \n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_layers)\n",
    "        self.fc = nn.Linear(d_model, output_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Embedding layers\n",
    "        src = self.embedding_src(src).unsqueeze(0)  # Add sequence dimension\n",
    "        tgt = self.embedding_tgt(tgt).unsqueeze(0)  # Add sequence dimension\n",
    "        \n",
    "        # Transformer\n",
    "        output = self.transformer(src, tgt)\n",
    "        \n",
    "        # Final layer to get the action sequence\n",
    "        return self.fc(output.squeeze(0))\n",
    "\n",
    "# Example usage:\n",
    "d_model = 512\n",
    "nhead = 16\n",
    "num_layers = 12\n",
    "\n",
    "model = TransformerNet(d_model, nhead, num_layers)\n",
    "\n",
    "src = torch.rand(input_size)\n",
    "tgt = torch.rand(output_size)\n",
    "action = model(src, tgt)\n",
    "print(action.shape)  # torch.Size([17])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e2cfd",
   "metadata": {},
   "source": [
    "## 2. ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47692387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from gym import spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae4a9f",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec11e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=512):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSACPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, observation_space: spaces.Space, action_space: spaces.Space, lr_schedule: Callable[[float], float], *args, **kwargs):\n",
    "        super(CustomSACPolicy, self).__init__(observation_space, action_space, lr_schedule, *args, **kwargs)\n",
    "        self._build_custom_extractor(lr_schedule)\n",
    "\n",
    "    def _build_custom_extractor(self, lr_schedule):\n",
    "        # Define custom actor network\n",
    "        self.actor = ResNet(self.observation_space.shape[0], self.action_space.shape[0])\n",
    "        # Define optimizer for the actor\n",
    "        self.actor.optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_schedule(1))\n",
    "        \n",
    "        # Define critic networks for SAC (Q-values)\n",
    "        self.critic = ResNet(self.observation_space.shape[0] + self.action_space.shape[0], 1)\n",
    "        self.critic.optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_schedule(1))\n",
    "        \n",
    "        self.critic_target = ResNet(self.observation_space.shape[0] + self.action_space.shape[0], 1)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, deterministic: bool = False):\n",
    "        # This method should return the action and the actor's latent\n",
    "        # Here we overwrite the default behavior to use our custom ResNet\n",
    "        return self.actor(obs), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1bf3655a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2e+03    |\n",
      "|    ep_rew_mean     | -447     |\n",
      "| time/              |          |\n",
      "|    fps             | 55       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2e+03       |\n",
      "|    ep_rew_mean          | -444        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014249872 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24.1       |\n",
      "|    explained_variance   | -0.0283     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0472      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 1.92        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2e+03       |\n",
      "|    ep_rew_mean          | -439        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 54          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011135392 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -24         |\n",
      "|    explained_variance   | 0.133       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0721      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 0.608       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x14e2de84d7b0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义策略训练模型\n",
    "model = PPO(CustomActorCriticPolicy, env, verbose=1)\n",
    "model.learn(500*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e5ed3",
   "metadata": {},
   "source": [
    "## SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "28b572cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from gym import spaces\n",
    "import torch as th\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=512):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.action_scale = 1.0\n",
    "        self.action_bias = 0.0\n",
    "\n",
    "    def action_log_prob(self, x):\n",
    "        # Get the logits for the actions\n",
    "        logits = self.network(x)\n",
    "        # Create a distribution from the logits\n",
    "        dist = th.distributions.Normal(logits, 1.0)\n",
    "        # Sample an action from the distribution\n",
    "        action = dist.rsample()\n",
    "        # Scale and shift the action to the desired range\n",
    "        action = th.tanh(action) * self.action_scale + self.action_bias\n",
    "        # Calculate the log probability of the action\n",
    "        log_prob = dist.log_prob(action).sum(axis=-1)\n",
    "        # Return the action and the log probability\n",
    "        return action, log_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=512):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.q1_network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.q2_network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, action):\n",
    "        q1 = self.q1_network(th.cat([obs, action], dim=1))\n",
    "        q2 = self.q2_network(th.cat([obs, action], dim=1))\n",
    "        return q1, q2\n",
    "\n",
    "\n",
    "class CustomSACPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, observation_space: spaces.Space, action_space: spaces.Space, lr_schedule: Callable[[float], float], *args, **kwargs):\n",
    "        super(CustomSACPolicy, self).__init__(observation_space, action_space, lr_schedule, *args, **kwargs)\n",
    "        self._build_custom_extractor(lr_schedule)\n",
    "\n",
    "    def _build_custom_extractor(self, lr_schedule):\n",
    "        # Define actor network\n",
    "        self.actor = ActorNetwork(self.observation_space.shape[0], self.action_space.shape[0])\n",
    "        self.actor.optimizer = th.optim.Adam(self.actor.parameters(), lr=lr_schedule(1))\n",
    "        \n",
    "        # Define critic networks for SAC (Q-values)\n",
    "        self.critic = CriticNetwork(self.observation_space.shape[0]+self.action_space.shape[0], 1)\n",
    "        self.critic.optimizer = th.optim.Adam(self.critic.parameters(), lr=lr_schedule(1))\n",
    "        \n",
    "        self.critic_target = CriticNetwork(self.observation_space.shape[0]+self.action_space.shape[0], 1)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "    def forward(self, obs: th.Tensor, deterministic: bool = False):\n",
    "        # This method should return the action and the actor's latent\n",
    "        # Here we overwrite the default behavior to use our custom ActorNetwork\n",
    "        action, log_prob = self.actor.action_log_prob(obs)\n",
    "        return action, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b07b7687",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -116      |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 34        |\n",
      "|    time_elapsed    | 58        |\n",
      "|    total_timesteps | 2000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.7e+23  |\n",
      "|    critic_loss     | inf       |\n",
      "|    ent_coef        | 0.847     |\n",
      "|    ent_coef_loss   | -1.71e+23 |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -116      |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 118       |\n",
      "|    total_timesteps | 4000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.34e+23 |\n",
      "|    critic_loss     | inf       |\n",
      "|    ent_coef        | 0.847     |\n",
      "|    ent_coef_loss   | -1.84e+23 |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -117      |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 177       |\n",
      "|    total_timesteps | 6000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.87e+23 |\n",
      "|    critic_loss     | inf       |\n",
      "|    ent_coef        | 0.847     |\n",
      "|    ent_coef_loss   | -1.94e+23 |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 5899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -117      |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 236       |\n",
      "|    total_timesteps | 8000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+24 |\n",
      "|    critic_loss     | inf       |\n",
      "|    ent_coef        | 0.847     |\n",
      "|    ent_coef_loss   | -2.02e+23 |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 7899      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 500       |\n",
      "|    ep_rew_mean     | -117      |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 296       |\n",
      "|    total_timesteps | 10000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+24 |\n",
      "|    critic_loss     | inf       |\n",
      "|    ent_coef        | 0.847     |\n",
      "|    ent_coef_loss   | -2.06e+23 |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 9899      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x14e2dc2c7a00>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用自定义策略训练SAC模型\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2)\n",
    "model = SAC(CustomSACPolicy, env=env, replay_buffer_class=ReplayBuffer, verbose=1, gamma = 0.9, batch_size=1000, \n",
    "            buffer_size=500000, learning_rate=0.001, action_noise=action_noise)\n",
    "model.learn(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309f9c8",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bf7a8fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (latent_pi): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (mu): Linear(in_features=512, out_features=14, bias=True)\n",
       "  (log_std): Linear(in_features=512, out_features=14, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_model = model.policy.actor.float()\n",
    "actor_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02d48883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContinuousCritic(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (qf0): Sequential(\n",
       "    (0): Linear(in_features=114, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (qf1): Sequential(\n",
       "    (0): Linear(in_features=114, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (qf2): Sequential(\n",
       "    (0): Linear(in_features=114, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (qf3): Sequential(\n",
       "    (0): Linear(in_features=114, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_model = model.policy.critic.float()\n",
    "critic_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f66185b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14])\n"
     ]
    }
   ],
   "source": [
    "# test actor model\n",
    "test_input = torch.ones(1, 100).to(device)\n",
    "test_output = actor_model(test_input)\n",
    "print(test_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908c5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic 1 Output: 0.0009697331115603447\n",
      "Critic 2 Output: 0.03320334851741791\n",
      "Critic 3 Output: -0.0352470688521862\n",
      "Critic 4 Output: -0.06494784355163574\n"
     ]
    }
   ],
   "source": [
    "# test critic model\n",
    "state_input = torch.ones(1, 100).to(device)\n",
    "action_input = torch.ones(1, 14).to(device)\n",
    "\n",
    "test_output = critic_model(state_input, action_input)\n",
    "for i, value in enumerate(test_output):\n",
    "    print(f\"Critic {i + 1} Output:\", value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da60264f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db50a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2084647e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc4384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123bbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ac83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d43e9477",
   "metadata": {},
   "source": [
    "# Human cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a826e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs shape: torch.Size([10, 3000, 100])\n",
      "action shape: torch.Size([10, 3000, 14])\n"
     ]
    }
   ],
   "source": [
    "class NPYFolder(Dataset):\n",
    "    def __init__(self, obs_dir, action_dir, transform=None):\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 使用glob匹配文件模式并获取所有的obs和action_OSC文件\n",
    "        self.obs_files = sorted(glob.glob(os.path.join(obs_dir, \"obs_seq_OSC_*\")))\n",
    "        self.action_files = sorted(glob.glob(os.path.join(action_dir, \"action_seq_OSC_*\")))\n",
    "        \n",
    "        assert len(self.obs_files) == len(self.action_files), \"Number of obs and action_OSC files must be the same!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.obs_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        obs = np.load(self.obs_files[idx])\n",
    "        action = np.load(self.action_files[idx])\n",
    "        \n",
    "        sample = {'obs': obs, 'action': action}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "obs_dir = \"./collectdata/obs\"\n",
    "action_dir = \"./collectdata/action_OSC\"\n",
    "dataset = NPYFolder(obs_dir, action_dir)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "sample_batch = next(iter(dataloader))\n",
    "obs_shape = sample_batch['obs'].shape\n",
    "action_shape = sample_batch['action'].shape\n",
    "print(f\"obs shape: {obs_shape}\")\n",
    "print(f\"action shape: {action_shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5eda99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(actor_model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "def train(actor_model, device, dataloader, loss_function, optimizer):\n",
    "    losses_train = []\n",
    "    optimizer.step()\n",
    "    actor_model.train()\n",
    "    for databatch in dataloader:\n",
    "        for n in range(databatch['obs'].shape[1]):\n",
    "            actor_model.zero_grad()\n",
    "            x = databatch['obs'][:,n,:].float().to(device)\n",
    "            y = databatch['action'][:,n,:].float().to(device)\n",
    "            output = actor_model.forward(x)\n",
    "            loss = loss_function(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses_train.append(loss.item())\n",
    "    return np.mean(losses_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4b43f53a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, Train Loss: 0.003\n",
      "EPOCH: 1, Train Loss: 0.003\n",
      "EPOCH: 2, Train Loss: 0.003\n",
      "EPOCH: 3, Train Loss: 0.003\n",
      "EPOCH: 4, Train Loss: 0.003\n",
      "EPOCH: 5, Train Loss: 0.003\n",
      "EPOCH: 6, Train Loss: 0.003\n",
      "EPOCH: 7, Train Loss: 0.003\n",
      "EPOCH: 8, Train Loss: 0.003\n",
      "EPOCH: 9, Train Loss: 0.003\n",
      "EPOCH: 10, Train Loss: 0.003\n",
      "EPOCH: 11, Train Loss: 0.003\n",
      "EPOCH: 12, Train Loss: 0.003\n",
      "EPOCH: 13, Train Loss: 0.003\n",
      "EPOCH: 14, Train Loss: 0.003\n",
      "EPOCH: 15, Train Loss: 0.003\n",
      "EPOCH: 16, Train Loss: 0.003\n",
      "EPOCH: 17, Train Loss: 0.003\n",
      "EPOCH: 18, Train Loss: 0.003\n",
      "EPOCH: 19, Train Loss: 0.003\n",
      "EPOCH: 20, Train Loss: 0.003\n",
      "EPOCH: 21, Train Loss: 0.003\n",
      "EPOCH: 22, Train Loss: 0.003\n",
      "EPOCH: 23, Train Loss: 0.003\n",
      "EPOCH: 24, Train Loss: 0.003\n",
      "EPOCH: 25, Train Loss: 0.003\n",
      "EPOCH: 26, Train Loss: 0.003\n",
      "EPOCH: 27, Train Loss: 0.003\n",
      "EPOCH: 28, Train Loss: 0.003\n",
      "EPOCH: 29, Train Loss: 0.003\n",
      "EPOCH: 30, Train Loss: 0.003\n",
      "EPOCH: 31, Train Loss: 0.003\n",
      "EPOCH: 32, Train Loss: 0.003\n",
      "EPOCH: 33, Train Loss: 0.003\n",
      "EPOCH: 34, Train Loss: 0.003\n",
      "EPOCH: 35, Train Loss: 0.003\n",
      "EPOCH: 36, Train Loss: 0.003\n",
      "EPOCH: 37, Train Loss: 0.003\n",
      "EPOCH: 38, Train Loss: 0.003\n",
      "EPOCH: 39, Train Loss: 0.003\n",
      "EPOCH: 40, Train Loss: 0.003\n",
      "EPOCH: 41, Train Loss: 0.003\n",
      "EPOCH: 42, Train Loss: 0.003\n",
      "EPOCH: 43, Train Loss: 0.003\n",
      "EPOCH: 44, Train Loss: 0.003\n",
      "EPOCH: 45, Train Loss: 0.003\n",
      "EPOCH: 46, Train Loss: 0.003\n",
      "EPOCH: 47, Train Loss: 0.003\n",
      "EPOCH: 48, Train Loss: 0.003\n",
      "EPOCH: 49, Train Loss: 0.003\n",
      "EPOCH: 50, Train Loss: 0.003\n",
      "EPOCH: 51, Train Loss: 0.003\n",
      "EPOCH: 52, Train Loss: 0.003\n",
      "EPOCH: 53, Train Loss: 0.003\n",
      "EPOCH: 54, Train Loss: 0.003\n",
      "EPOCH: 55, Train Loss: 0.003\n",
      "EPOCH: 56, Train Loss: 0.003\n",
      "EPOCH: 57, Train Loss: 0.003\n",
      "EPOCH: 58, Train Loss: 0.003\n",
      "EPOCH: 59, Train Loss: 0.003\n",
      "EPOCH: 60, Train Loss: 0.003\n",
      "EPOCH: 61, Train Loss: 0.003\n",
      "EPOCH: 62, Train Loss: 0.003\n",
      "EPOCH: 63, Train Loss: 0.003\n",
      "EPOCH: 64, Train Loss: 0.003\n",
      "EPOCH: 65, Train Loss: 0.003\n",
      "EPOCH: 66, Train Loss: 0.003\n",
      "EPOCH: 67, Train Loss: 0.003\n",
      "EPOCH: 68, Train Loss: 0.003\n",
      "EPOCH: 69, Train Loss: 0.003\n",
      "EPOCH: 70, Train Loss: 0.003\n",
      "EPOCH: 71, Train Loss: 0.003\n",
      "EPOCH: 72, Train Loss: 0.003\n",
      "EPOCH: 73, Train Loss: 0.003\n",
      "EPOCH: 74, Train Loss: 0.003\n",
      "EPOCH: 75, Train Loss: 0.003\n",
      "EPOCH: 76, Train Loss: 0.003\n",
      "EPOCH: 77, Train Loss: 0.003\n",
      "EPOCH: 78, Train Loss: 0.003\n",
      "EPOCH: 79, Train Loss: 0.003\n",
      "EPOCH: 80, Train Loss: 0.003\n",
      "EPOCH: 81, Train Loss: 0.003\n",
      "EPOCH: 82, Train Loss: 0.003\n",
      "EPOCH: 83, Train Loss: 0.003\n",
      "EPOCH: 84, Train Loss: 0.003\n",
      "EPOCH: 85, Train Loss: 0.003\n",
      "EPOCH: 86, Train Loss: 0.003\n",
      "EPOCH: 87, Train Loss: 0.003\n",
      "EPOCH: 88, Train Loss: 0.003\n",
      "EPOCH: 89, Train Loss: 0.003\n",
      "EPOCH: 90, Train Loss: 0.003\n",
      "EPOCH: 91, Train Loss: 0.003\n",
      "EPOCH: 92, Train Loss: 0.003\n",
      "EPOCH: 93, Train Loss: 0.003\n",
      "EPOCH: 94, Train Loss: 0.003\n",
      "EPOCH: 95, Train Loss: 0.003\n",
      "EPOCH: 96, Train Loss: 0.003\n",
      "EPOCH: 97, Train Loss: 0.003\n",
      "EPOCH: 98, Train Loss: 0.003\n",
      "EPOCH: 99, Train Loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = train(actor_model, device, dataloader, loss_function, optimizer)\n",
    "    scheduler.step()\n",
    "    print('EPOCH: {}, Train Loss: {:.3f}'.format(epoch, loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8e60e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./models/maholo_SAC_actor_weights.pth\n"
     ]
    }
   ],
   "source": [
    "weightpath = \"./models/maholo_SAC_OSC_Actor.pth\"\n",
    "torch.save(actor_model.state_dict(), weightpath)\n",
    "print(\"Saved to\", savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145b26c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23026494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightpath = \"./models/maholo_SAC_OSC.pth\"\n",
    "policy_kwargs = {'net_arch' : [512, 512, 512, 512], \n",
    "                'n_critics' : 4,\n",
    "                }\n",
    "model = SAC(policy=\"MlpPolicy\", env=env, policy_kwargs=policy_kwargs)\n",
    "# model.policy.actor.load_state_dict(torch.load(weightpath))\n",
    "model.policy.load_state_dict(torch.load(weightpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81712a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5073a694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63c938f0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088d246",
   "metadata": {},
   "source": [
    "# Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a4e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = suite.make(\n",
    "    env_name=\"Lift\",\n",
    "    robots=\"Panda\",\n",
    "    has_renderer=True,\n",
    "    has_offscreen_renderer=True,\n",
    "    use_camera_obs=False,\n",
    "    control_freq=50,\n",
    "    horizon = 50,\n",
    ")\n",
    "env = GymWrapper(env)\n",
    "env = TimeFeatureWrapper(env)\n",
    "model = DDPG.load(modelpath, env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff41f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.unwrapped.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d01bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
