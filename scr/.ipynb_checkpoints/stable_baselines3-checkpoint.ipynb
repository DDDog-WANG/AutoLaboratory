{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8589b5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import robosuite as suite\n",
    "from robosuite.wrappers.gym_wrapper import GymWrapper\n",
    "import numpy as np\n",
    "from stable_baselines3 import DDPG , SAC, PPO\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from sb3_contrib.common.wrappers import TimeFeatureWrapper\n",
    "import argparse\n",
    "modelpath = './models/PPO_3.0'\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,TensorDataset,random_split,DataLoader,SubsetRandomSampler\n",
    "from torch.utils.data.dataset import Subset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d349c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.device(cuda)\n",
      "torch.cuda.device_count():  1\n",
      "Tesla V100-SXM2-16GB\n",
      "torch.cuda.current_device() 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"torch.device(cuda)\")\n",
    "    print(\"torch.cuda.device_count(): \", torch.cuda.device_count())\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(torch.cuda.get_device_name())\n",
    "    print(\"torch.cuda.current_device()\", torch.cuda.current_device())\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"torch.device(cpu)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444921b",
   "metadata": {},
   "source": [
    "# Load Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa62456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: robot0_joint_pos_cos, Value.shape: (15,)\n",
      "Key: robot0_joint_pos_sin, Value.shape: (15,)\n",
      "Key: robot0_joint_vel, Value.shape: (15,)\n",
      "Key: robot0_right_eef_pos, Value.shape: (3,)\n",
      "Key: robot0_right_eef_quat, Value.shape: (4,)\n",
      "Key: robot0_right_gripper_qpos, Value.shape: (2,)\n",
      "Key: robot0_right_gripper_qvel, Value.shape: (2,)\n",
      "Key: robot0_left_eef_pos, Value.shape: (3,)\n",
      "Key: robot0_left_eef_quat, Value.shape: (4,)\n",
      "Key: robot0_left_gripper_qpos, Value.shape: (2,)\n",
      "Key: robot0_left_gripper_qvel, Value.shape: (2,)\n",
      "Key: tube_initial_pos, Value.shape: (3,)\n",
      "Key: pipette_initial_pos, Value.shape: (3,)\n",
      "Key: pipette004_pos, Value.shape: (3,)\n",
      "Key: pipette004_quat, Value.shape: (4,)\n",
      "Key: tube008_pos, Value.shape: (3,)\n",
      "Key: tube008_quat, Value.shape: (4,)\n",
      "Key: gripper1_to_pipette004, Value.shape: (3,)\n",
      "Key: pipette004_to_tube008, Value.shape: (3,)\n",
      "Key: tube008_to_initial, Value.shape: (3,)\n",
      "Key: pipette004_to_initial, Value.shape: (3,)\n",
      "Key: robot0_proprio-state, Value.shape: (67,)\n",
      "Key: object-state, Value.shape: (32,)\n"
     ]
    }
   ],
   "source": [
    "env = suite.make(\n",
    "    \"MaholoLaboratory\",\n",
    "    \"Maholo\",\n",
    "    has_renderer=False,\n",
    "    has_offscreen_renderer=False,\n",
    "    use_camera_obs=False,\n",
    "    control_freq=50,\n",
    "    render_camera=\"frontview\",\n",
    "    render_gpu_device_id=0,\n",
    "    horizon=400,\n",
    ")\n",
    "for key,value in env.reset().items():\n",
    "    print(f\"Key: {key}, Value.shape: {value.shape}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2046ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acd13264yb/robosuite_env/lib/python3.10/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeFeature GYM Wrapper obs.shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "env = GymWrapper(env)\n",
    "env = TimeFeatureWrapper(env)\n",
    "print(f\"TimeFeature GYM Wrapper obs.shape: {env.reset().shape}\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309f9c8",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19415bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_actions: 17\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"SAC\"\n",
    "learning_rate = 0.001\n",
    "total_timesteps = 400*1\n",
    "policy_kwargs = {'net_arch' : [512, 512, 512, 512], \n",
    "                'n_critics' : 4,\n",
    "                }\n",
    "n_actions = env.robots[0].action_dim\n",
    "print(f\"n_actions: {n_actions}\", flush=True)\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2)\n",
    "\n",
    "if model_name == \"DDPG\":\n",
    "    model = DDPG(policy=\"MlpPolicy\", env=env, replay_buffer_class=ReplayBuffer, verbose=1, gamma=0.95, batch_size=4096, \n",
    "                buffer_size=100000, learning_rate=learning_rate, action_noise=action_noise, policy_kwargs=policy_kwargs)\n",
    "elif model_name == \"SAC\":\n",
    "    model = SAC(policy=\"MlpPolicy\", env=env, replay_buffer_class=ReplayBuffer, verbose=1, gamma = 0.95, batch_size=4096, \n",
    "                buffer_size=100000, learning_rate=learning_rate, action_noise=action_noise, policy_kwargs=policy_kwargs)\n",
    "elif model_name == \"PPO\":\n",
    "    model = PPO(policy=\"MlpPolicy\", env=env, verbose=1, gamma=0.95, batch_size=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf7a8fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Actor(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (latent_pi): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (mu): Linear(in_features=512, out_features=17, bias=True)\n",
       "  (log_std): Linear(in_features=512, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_model = model.policy.actor\n",
    "actor_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d48883",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContinuousCritic(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (qf0): Sequential(\n",
       "    (0): Linear(in_features=117, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (qf1): Sequential(\n",
       "    (0): Linear(in_features=117, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (qf2): Sequential(\n",
       "    (0): Linear(in_features=117, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (qf3): Sequential(\n",
       "    (0): Linear(in_features=117, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic_model = model.policy.critic\n",
    "critic_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66185b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17])\n"
     ]
    }
   ],
   "source": [
    "# test actor model\n",
    "test_input = torch.ones(1, 100).to(device)\n",
    "test_output = actor_model(test_input)\n",
    "print(test_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "908c5859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic 1 Output: 0.0030027865432202816\n",
      "Critic 2 Output: 0.02625722438097\n",
      "Critic 3 Output: 0.008443258702754974\n",
      "Critic 4 Output: -0.03417135775089264\n"
     ]
    }
   ],
   "source": [
    "# test critic model\n",
    "state_input = torch.ones(1, 100).to(device)\n",
    "action_input = torch.ones(1, 17).to(device)\n",
    "\n",
    "test_output = critic_model(state_input, action_input)\n",
    "for i, value in enumerate(test_output):\n",
    "    print(f\"Critic {i + 1} Output:\", value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43e9477",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47ca2e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 17)\n",
      "(10, 100)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "action_seq = np.load(\"./collectdata/action_seq.npy\")\n",
    "obs_seq = np.load(\"./collectdata/obs_seq.npy\")\n",
    "reward_seq = np.load(\"./collectdata/reward_seq.npy\")\n",
    "print(action_seq.shape)\n",
    "print(obs_seq.shape)\n",
    "print(reward_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "438a2b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class set_dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "train_data = set_dataset(obs_seq, action_seq)\n",
    "dataloader_train = DataLoader(train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eda99a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(actor_model.parameters(), lr=0.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n",
    "\n",
    "def train(actor_model, device, dataloader_train, loss_function, optimizer):\n",
    "    losses_train = []\n",
    "    optimizer.step()\n",
    "    actor_model.train()\n",
    "    for x, y in dataloader_train:\n",
    "        actor_model.zero_grad()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        output = actor_model.forward(x)\n",
    "        loss = loss_function(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_train.append(loss.item())\n",
    "    return np.mean(losses_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b43f53a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, Train Loss: 0.602\n",
      "EPOCH: 1, Train Loss: 0.582\n",
      "EPOCH: 2, Train Loss: 0.647\n",
      "EPOCH: 3, Train Loss: 0.533\n",
      "EPOCH: 4, Train Loss: 0.705\n",
      "EPOCH: 5, Train Loss: 0.653\n",
      "EPOCH: 6, Train Loss: 0.618\n",
      "EPOCH: 7, Train Loss: 0.635\n",
      "EPOCH: 8, Train Loss: 0.659\n",
      "EPOCH: 9, Train Loss: 0.636\n",
      "EPOCH: 10, Train Loss: 0.610\n",
      "EPOCH: 11, Train Loss: 0.733\n",
      "EPOCH: 12, Train Loss: 0.604\n",
      "EPOCH: 13, Train Loss: 0.625\n",
      "EPOCH: 14, Train Loss: 0.604\n",
      "EPOCH: 15, Train Loss: 0.637\n",
      "EPOCH: 16, Train Loss: 0.646\n",
      "EPOCH: 17, Train Loss: 0.629\n",
      "EPOCH: 18, Train Loss: 0.597\n",
      "EPOCH: 19, Train Loss: 0.700\n",
      "EPOCH: 20, Train Loss: 0.533\n",
      "EPOCH: 21, Train Loss: 0.611\n",
      "EPOCH: 22, Train Loss: 0.613\n",
      "EPOCH: 23, Train Loss: 0.579\n",
      "EPOCH: 24, Train Loss: 0.687\n",
      "EPOCH: 25, Train Loss: 0.583\n",
      "EPOCH: 26, Train Loss: 0.728\n",
      "EPOCH: 27, Train Loss: 0.763\n",
      "EPOCH: 28, Train Loss: 0.687\n",
      "EPOCH: 29, Train Loss: 0.775\n",
      "EPOCH: 30, Train Loss: 0.692\n",
      "EPOCH: 31, Train Loss: 0.558\n",
      "EPOCH: 32, Train Loss: 0.769\n",
      "EPOCH: 33, Train Loss: 0.549\n",
      "EPOCH: 34, Train Loss: 0.541\n",
      "EPOCH: 35, Train Loss: 0.590\n",
      "EPOCH: 36, Train Loss: 0.764\n",
      "EPOCH: 37, Train Loss: 0.659\n",
      "EPOCH: 38, Train Loss: 0.634\n",
      "EPOCH: 39, Train Loss: 0.692\n",
      "EPOCH: 40, Train Loss: 0.613\n",
      "EPOCH: 41, Train Loss: 0.562\n",
      "EPOCH: 42, Train Loss: 0.674\n",
      "EPOCH: 43, Train Loss: 0.615\n",
      "EPOCH: 44, Train Loss: 0.564\n",
      "EPOCH: 45, Train Loss: 0.587\n",
      "EPOCH: 46, Train Loss: 0.577\n",
      "EPOCH: 47, Train Loss: 0.578\n",
      "EPOCH: 48, Train Loss: 0.643\n",
      "EPOCH: 49, Train Loss: 0.597\n",
      "EPOCH: 50, Train Loss: 0.664\n",
      "EPOCH: 51, Train Loss: 0.733\n",
      "EPOCH: 52, Train Loss: 0.542\n",
      "EPOCH: 53, Train Loss: 0.572\n",
      "EPOCH: 54, Train Loss: 0.467\n",
      "EPOCH: 55, Train Loss: 0.691\n",
      "EPOCH: 56, Train Loss: 0.593\n",
      "EPOCH: 57, Train Loss: 0.677\n",
      "EPOCH: 58, Train Loss: 0.663\n",
      "EPOCH: 59, Train Loss: 0.682\n",
      "EPOCH: 60, Train Loss: 0.632\n",
      "EPOCH: 61, Train Loss: 0.637\n",
      "EPOCH: 62, Train Loss: 0.539\n",
      "EPOCH: 63, Train Loss: 0.567\n",
      "EPOCH: 64, Train Loss: 0.632\n",
      "EPOCH: 65, Train Loss: 0.598\n",
      "EPOCH: 66, Train Loss: 0.680\n",
      "EPOCH: 67, Train Loss: 0.629\n",
      "EPOCH: 68, Train Loss: 0.548\n",
      "EPOCH: 69, Train Loss: 0.607\n",
      "EPOCH: 70, Train Loss: 0.591\n",
      "EPOCH: 71, Train Loss: 0.621\n",
      "EPOCH: 72, Train Loss: 0.582\n",
      "EPOCH: 73, Train Loss: 0.521\n",
      "EPOCH: 74, Train Loss: 0.597\n",
      "EPOCH: 75, Train Loss: 0.609\n",
      "EPOCH: 76, Train Loss: 0.608\n",
      "EPOCH: 77, Train Loss: 0.654\n",
      "EPOCH: 78, Train Loss: 0.604\n",
      "EPOCH: 79, Train Loss: 0.697\n",
      "EPOCH: 80, Train Loss: 0.582\n",
      "EPOCH: 81, Train Loss: 0.565\n",
      "EPOCH: 82, Train Loss: 0.642\n",
      "EPOCH: 83, Train Loss: 0.665\n",
      "EPOCH: 84, Train Loss: 0.588\n",
      "EPOCH: 85, Train Loss: 0.583\n",
      "EPOCH: 86, Train Loss: 0.590\n",
      "EPOCH: 87, Train Loss: 0.636\n",
      "EPOCH: 88, Train Loss: 0.600\n",
      "EPOCH: 89, Train Loss: 0.494\n",
      "EPOCH: 90, Train Loss: 0.555\n",
      "EPOCH: 91, Train Loss: 0.673\n",
      "EPOCH: 92, Train Loss: 0.540\n",
      "EPOCH: 93, Train Loss: 0.586\n",
      "EPOCH: 94, Train Loss: 0.621\n",
      "EPOCH: 95, Train Loss: 0.564\n",
      "EPOCH: 96, Train Loss: 0.576\n",
      "EPOCH: 97, Train Loss: 0.507\n",
      "EPOCH: 98, Train Loss: 0.580\n",
      "EPOCH: 99, Train Loss: 0.467\n",
      "EPOCH: 100, Train Loss: 0.563\n",
      "EPOCH: 101, Train Loss: 0.510\n",
      "EPOCH: 102, Train Loss: 0.616\n",
      "EPOCH: 103, Train Loss: 0.581\n",
      "EPOCH: 104, Train Loss: 0.495\n",
      "EPOCH: 105, Train Loss: 0.520\n",
      "EPOCH: 106, Train Loss: 0.617\n",
      "EPOCH: 107, Train Loss: 0.555\n",
      "EPOCH: 108, Train Loss: 0.589\n",
      "EPOCH: 109, Train Loss: 0.517\n",
      "EPOCH: 110, Train Loss: 0.546\n",
      "EPOCH: 111, Train Loss: 0.491\n",
      "EPOCH: 112, Train Loss: 0.508\n",
      "EPOCH: 113, Train Loss: 0.582\n",
      "EPOCH: 114, Train Loss: 0.501\n",
      "EPOCH: 115, Train Loss: 0.591\n",
      "EPOCH: 116, Train Loss: 0.536\n",
      "EPOCH: 117, Train Loss: 0.576\n",
      "EPOCH: 118, Train Loss: 0.596\n",
      "EPOCH: 119, Train Loss: 0.479\n",
      "EPOCH: 120, Train Loss: 0.561\n",
      "EPOCH: 121, Train Loss: 0.551\n",
      "EPOCH: 122, Train Loss: 0.547\n",
      "EPOCH: 123, Train Loss: 0.508\n",
      "EPOCH: 124, Train Loss: 0.602\n",
      "EPOCH: 125, Train Loss: 0.512\n",
      "EPOCH: 126, Train Loss: 0.503\n",
      "EPOCH: 127, Train Loss: 0.491\n",
      "EPOCH: 128, Train Loss: 0.512\n",
      "EPOCH: 129, Train Loss: 0.452\n",
      "EPOCH: 130, Train Loss: 0.616\n",
      "EPOCH: 131, Train Loss: 0.637\n",
      "EPOCH: 132, Train Loss: 0.616\n",
      "EPOCH: 133, Train Loss: 0.477\n",
      "EPOCH: 134, Train Loss: 0.551\n",
      "EPOCH: 135, Train Loss: 0.540\n",
      "EPOCH: 136, Train Loss: 0.571\n",
      "EPOCH: 137, Train Loss: 0.532\n",
      "EPOCH: 138, Train Loss: 0.592\n",
      "EPOCH: 139, Train Loss: 0.509\n",
      "EPOCH: 140, Train Loss: 0.445\n",
      "EPOCH: 141, Train Loss: 0.650\n",
      "EPOCH: 142, Train Loss: 0.530\n",
      "EPOCH: 143, Train Loss: 0.547\n",
      "EPOCH: 144, Train Loss: 0.525\n",
      "EPOCH: 145, Train Loss: 0.480\n",
      "EPOCH: 146, Train Loss: 0.530\n",
      "EPOCH: 147, Train Loss: 0.521\n",
      "EPOCH: 148, Train Loss: 0.511\n",
      "EPOCH: 149, Train Loss: 0.555\n",
      "EPOCH: 150, Train Loss: 0.638\n",
      "EPOCH: 151, Train Loss: 0.597\n",
      "EPOCH: 152, Train Loss: 0.630\n",
      "EPOCH: 153, Train Loss: 0.617\n",
      "EPOCH: 154, Train Loss: 0.487\n",
      "EPOCH: 155, Train Loss: 0.498\n",
      "EPOCH: 156, Train Loss: 0.526\n",
      "EPOCH: 157, Train Loss: 0.516\n",
      "EPOCH: 158, Train Loss: 0.500\n",
      "EPOCH: 159, Train Loss: 0.499\n",
      "EPOCH: 160, Train Loss: 0.563\n",
      "EPOCH: 161, Train Loss: 0.583\n",
      "EPOCH: 162, Train Loss: 0.533\n",
      "EPOCH: 163, Train Loss: 0.459\n",
      "EPOCH: 164, Train Loss: 0.518\n",
      "EPOCH: 165, Train Loss: 0.578\n",
      "EPOCH: 166, Train Loss: 0.570\n",
      "EPOCH: 167, Train Loss: 0.453\n",
      "EPOCH: 168, Train Loss: 0.472\n",
      "EPOCH: 169, Train Loss: 0.526\n",
      "EPOCH: 170, Train Loss: 0.569\n",
      "EPOCH: 171, Train Loss: 0.547\n",
      "EPOCH: 172, Train Loss: 0.402\n",
      "EPOCH: 173, Train Loss: 0.591\n",
      "EPOCH: 174, Train Loss: 0.563\n",
      "EPOCH: 175, Train Loss: 0.494\n",
      "EPOCH: 176, Train Loss: 0.566\n",
      "EPOCH: 177, Train Loss: 0.494\n",
      "EPOCH: 178, Train Loss: 0.474\n",
      "EPOCH: 179, Train Loss: 0.617\n",
      "EPOCH: 180, Train Loss: 0.496\n",
      "EPOCH: 181, Train Loss: 0.478\n",
      "EPOCH: 182, Train Loss: 0.463\n",
      "EPOCH: 183, Train Loss: 0.566\n",
      "EPOCH: 184, Train Loss: 0.483\n",
      "EPOCH: 185, Train Loss: 0.580\n",
      "EPOCH: 186, Train Loss: 0.560\n",
      "EPOCH: 187, Train Loss: 0.564\n",
      "EPOCH: 188, Train Loss: 0.603\n",
      "EPOCH: 189, Train Loss: 0.444\n",
      "EPOCH: 190, Train Loss: 0.527\n",
      "EPOCH: 191, Train Loss: 0.490\n",
      "EPOCH: 192, Train Loss: 0.495\n",
      "EPOCH: 193, Train Loss: 0.653\n",
      "EPOCH: 194, Train Loss: 0.621\n",
      "EPOCH: 195, Train Loss: 0.588\n",
      "EPOCH: 196, Train Loss: 0.571\n",
      "EPOCH: 197, Train Loss: 0.518\n",
      "EPOCH: 198, Train Loss: 0.480\n",
      "EPOCH: 199, Train Loss: 0.556\n",
      "EPOCH: 200, Train Loss: 0.567\n",
      "EPOCH: 201, Train Loss: 0.558\n",
      "EPOCH: 202, Train Loss: 0.488\n",
      "EPOCH: 203, Train Loss: 0.508\n",
      "EPOCH: 204, Train Loss: 0.582\n",
      "EPOCH: 205, Train Loss: 0.610\n",
      "EPOCH: 206, Train Loss: 0.504\n",
      "EPOCH: 207, Train Loss: 0.571\n",
      "EPOCH: 208, Train Loss: 0.544\n",
      "EPOCH: 209, Train Loss: 0.554\n",
      "EPOCH: 210, Train Loss: 0.516\n",
      "EPOCH: 211, Train Loss: 0.558\n",
      "EPOCH: 212, Train Loss: 0.633\n",
      "EPOCH: 213, Train Loss: 0.416\n",
      "EPOCH: 214, Train Loss: 0.506\n",
      "EPOCH: 215, Train Loss: 0.506\n",
      "EPOCH: 216, Train Loss: 0.560\n",
      "EPOCH: 217, Train Loss: 0.497\n",
      "EPOCH: 218, Train Loss: 0.492\n",
      "EPOCH: 219, Train Loss: 0.503\n",
      "EPOCH: 220, Train Loss: 0.422\n",
      "EPOCH: 221, Train Loss: 0.487\n",
      "EPOCH: 222, Train Loss: 0.556\n",
      "EPOCH: 223, Train Loss: 0.532\n",
      "EPOCH: 224, Train Loss: 0.450\n",
      "EPOCH: 225, Train Loss: 0.480\n",
      "EPOCH: 226, Train Loss: 0.435\n",
      "EPOCH: 227, Train Loss: 0.590\n",
      "EPOCH: 228, Train Loss: 0.531\n",
      "EPOCH: 229, Train Loss: 0.617\n",
      "EPOCH: 230, Train Loss: 0.596\n",
      "EPOCH: 231, Train Loss: 0.534\n",
      "EPOCH: 232, Train Loss: 0.521\n",
      "EPOCH: 233, Train Loss: 0.564\n",
      "EPOCH: 234, Train Loss: 0.541\n",
      "EPOCH: 235, Train Loss: 0.502\n",
      "EPOCH: 236, Train Loss: 0.542\n",
      "EPOCH: 237, Train Loss: 0.554\n",
      "EPOCH: 238, Train Loss: 0.444\n",
      "EPOCH: 239, Train Loss: 0.533\n",
      "EPOCH: 240, Train Loss: 0.539\n",
      "EPOCH: 241, Train Loss: 0.499\n",
      "EPOCH: 242, Train Loss: 0.534\n",
      "EPOCH: 243, Train Loss: 0.542\n",
      "EPOCH: 244, Train Loss: 0.551\n",
      "EPOCH: 245, Train Loss: 0.515\n",
      "EPOCH: 246, Train Loss: 0.508\n",
      "EPOCH: 247, Train Loss: 0.503\n",
      "EPOCH: 248, Train Loss: 0.591\n",
      "EPOCH: 249, Train Loss: 0.529\n",
      "EPOCH: 250, Train Loss: 0.511\n",
      "EPOCH: 251, Train Loss: 0.495\n",
      "EPOCH: 252, Train Loss: 0.569\n",
      "EPOCH: 253, Train Loss: 0.542\n",
      "EPOCH: 254, Train Loss: 0.495\n",
      "EPOCH: 255, Train Loss: 0.525\n",
      "EPOCH: 256, Train Loss: 0.490\n",
      "EPOCH: 257, Train Loss: 0.538\n",
      "EPOCH: 258, Train Loss: 0.479\n",
      "EPOCH: 259, Train Loss: 0.421\n",
      "EPOCH: 260, Train Loss: 0.631\n",
      "EPOCH: 261, Train Loss: 0.526\n",
      "EPOCH: 262, Train Loss: 0.512\n",
      "EPOCH: 263, Train Loss: 0.452\n",
      "EPOCH: 264, Train Loss: 0.580\n",
      "EPOCH: 265, Train Loss: 0.515\n",
      "EPOCH: 266, Train Loss: 0.527\n",
      "EPOCH: 267, Train Loss: 0.489\n",
      "EPOCH: 268, Train Loss: 0.468\n",
      "EPOCH: 269, Train Loss: 0.455\n",
      "EPOCH: 270, Train Loss: 0.494\n",
      "EPOCH: 271, Train Loss: 0.562\n",
      "EPOCH: 272, Train Loss: 0.497\n",
      "EPOCH: 273, Train Loss: 0.501\n",
      "EPOCH: 274, Train Loss: 0.522\n",
      "EPOCH: 275, Train Loss: 0.522\n",
      "EPOCH: 276, Train Loss: 0.507\n",
      "EPOCH: 277, Train Loss: 0.584\n",
      "EPOCH: 278, Train Loss: 0.419\n",
      "EPOCH: 279, Train Loss: 0.503\n",
      "EPOCH: 280, Train Loss: 0.549\n",
      "EPOCH: 281, Train Loss: 0.476\n",
      "EPOCH: 282, Train Loss: 0.429\n",
      "EPOCH: 283, Train Loss: 0.572\n",
      "EPOCH: 284, Train Loss: 0.567\n",
      "EPOCH: 285, Train Loss: 0.503\n",
      "EPOCH: 286, Train Loss: 0.528\n",
      "EPOCH: 287, Train Loss: 0.482\n",
      "EPOCH: 288, Train Loss: 0.580\n",
      "EPOCH: 289, Train Loss: 0.492\n",
      "EPOCH: 290, Train Loss: 0.547\n",
      "EPOCH: 291, Train Loss: 0.549\n",
      "EPOCH: 292, Train Loss: 0.466\n",
      "EPOCH: 293, Train Loss: 0.552\n",
      "EPOCH: 294, Train Loss: 0.551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 295, Train Loss: 0.570\n",
      "EPOCH: 296, Train Loss: 0.510\n",
      "EPOCH: 297, Train Loss: 0.494\n",
      "EPOCH: 298, Train Loss: 0.448\n",
      "EPOCH: 299, Train Loss: 0.552\n",
      "EPOCH: 300, Train Loss: 0.515\n",
      "EPOCH: 301, Train Loss: 0.545\n",
      "EPOCH: 302, Train Loss: 0.576\n",
      "EPOCH: 303, Train Loss: 0.449\n",
      "EPOCH: 304, Train Loss: 0.551\n",
      "EPOCH: 305, Train Loss: 0.468\n",
      "EPOCH: 306, Train Loss: 0.530\n",
      "EPOCH: 307, Train Loss: 0.554\n",
      "EPOCH: 308, Train Loss: 0.541\n",
      "EPOCH: 309, Train Loss: 0.571\n",
      "EPOCH: 310, Train Loss: 0.602\n",
      "EPOCH: 311, Train Loss: 0.458\n",
      "EPOCH: 312, Train Loss: 0.473\n",
      "EPOCH: 313, Train Loss: 0.523\n",
      "EPOCH: 314, Train Loss: 0.548\n",
      "EPOCH: 315, Train Loss: 0.582\n",
      "EPOCH: 316, Train Loss: 0.508\n",
      "EPOCH: 317, Train Loss: 0.494\n",
      "EPOCH: 318, Train Loss: 0.511\n",
      "EPOCH: 319, Train Loss: 0.606\n",
      "EPOCH: 320, Train Loss: 0.500\n",
      "EPOCH: 321, Train Loss: 0.544\n",
      "EPOCH: 322, Train Loss: 0.526\n",
      "EPOCH: 323, Train Loss: 0.477\n",
      "EPOCH: 324, Train Loss: 0.476\n",
      "EPOCH: 325, Train Loss: 0.534\n",
      "EPOCH: 326, Train Loss: 0.518\n",
      "EPOCH: 327, Train Loss: 0.604\n",
      "EPOCH: 328, Train Loss: 0.517\n",
      "EPOCH: 329, Train Loss: 0.561\n",
      "EPOCH: 330, Train Loss: 0.558\n",
      "EPOCH: 331, Train Loss: 0.506\n",
      "EPOCH: 332, Train Loss: 0.468\n",
      "EPOCH: 333, Train Loss: 0.515\n",
      "EPOCH: 334, Train Loss: 0.500\n",
      "EPOCH: 335, Train Loss: 0.464\n",
      "EPOCH: 336, Train Loss: 0.545\n",
      "EPOCH: 337, Train Loss: 0.597\n",
      "EPOCH: 338, Train Loss: 0.465\n",
      "EPOCH: 339, Train Loss: 0.531\n",
      "EPOCH: 340, Train Loss: 0.515\n",
      "EPOCH: 341, Train Loss: 0.512\n",
      "EPOCH: 342, Train Loss: 0.525\n",
      "EPOCH: 343, Train Loss: 0.456\n",
      "EPOCH: 344, Train Loss: 0.495\n",
      "EPOCH: 345, Train Loss: 0.510\n",
      "EPOCH: 346, Train Loss: 0.529\n",
      "EPOCH: 347, Train Loss: 0.464\n",
      "EPOCH: 348, Train Loss: 0.494\n",
      "EPOCH: 349, Train Loss: 0.538\n",
      "EPOCH: 350, Train Loss: 0.582\n",
      "EPOCH: 351, Train Loss: 0.577\n",
      "EPOCH: 352, Train Loss: 0.551\n",
      "EPOCH: 353, Train Loss: 0.516\n",
      "EPOCH: 354, Train Loss: 0.504\n",
      "EPOCH: 355, Train Loss: 0.500\n",
      "EPOCH: 356, Train Loss: 0.580\n",
      "EPOCH: 357, Train Loss: 0.516\n",
      "EPOCH: 358, Train Loss: 0.577\n",
      "EPOCH: 359, Train Loss: 0.516\n",
      "EPOCH: 360, Train Loss: 0.467\n",
      "EPOCH: 361, Train Loss: 0.484\n",
      "EPOCH: 362, Train Loss: 0.536\n",
      "EPOCH: 363, Train Loss: 0.585\n",
      "EPOCH: 364, Train Loss: 0.473\n",
      "EPOCH: 365, Train Loss: 0.638\n",
      "EPOCH: 366, Train Loss: 0.474\n",
      "EPOCH: 367, Train Loss: 0.555\n",
      "EPOCH: 368, Train Loss: 0.512\n",
      "EPOCH: 369, Train Loss: 0.475\n",
      "EPOCH: 370, Train Loss: 0.564\n",
      "EPOCH: 371, Train Loss: 0.561\n",
      "EPOCH: 372, Train Loss: 0.457\n",
      "EPOCH: 373, Train Loss: 0.478\n",
      "EPOCH: 374, Train Loss: 0.395\n",
      "EPOCH: 375, Train Loss: 0.446\n",
      "EPOCH: 376, Train Loss: 0.517\n",
      "EPOCH: 377, Train Loss: 0.414\n",
      "EPOCH: 378, Train Loss: 0.512\n",
      "EPOCH: 379, Train Loss: 0.466\n",
      "EPOCH: 380, Train Loss: 0.514\n",
      "EPOCH: 381, Train Loss: 0.544\n",
      "EPOCH: 382, Train Loss: 0.495\n",
      "EPOCH: 383, Train Loss: 0.461\n",
      "EPOCH: 384, Train Loss: 0.547\n",
      "EPOCH: 385, Train Loss: 0.496\n",
      "EPOCH: 386, Train Loss: 0.439\n",
      "EPOCH: 387, Train Loss: 0.456\n",
      "EPOCH: 388, Train Loss: 0.494\n",
      "EPOCH: 389, Train Loss: 0.502\n",
      "EPOCH: 390, Train Loss: 0.478\n",
      "EPOCH: 391, Train Loss: 0.465\n",
      "EPOCH: 392, Train Loss: 0.537\n",
      "EPOCH: 393, Train Loss: 0.502\n",
      "EPOCH: 394, Train Loss: 0.516\n",
      "EPOCH: 395, Train Loss: 0.447\n",
      "EPOCH: 396, Train Loss: 0.531\n",
      "EPOCH: 397, Train Loss: 0.493\n",
      "EPOCH: 398, Train Loss: 0.535\n",
      "EPOCH: 399, Train Loss: 0.511\n",
      "EPOCH: 400, Train Loss: 0.501\n",
      "EPOCH: 401, Train Loss: 0.490\n",
      "EPOCH: 402, Train Loss: 0.449\n",
      "EPOCH: 403, Train Loss: 0.469\n",
      "EPOCH: 404, Train Loss: 0.525\n",
      "EPOCH: 405, Train Loss: 0.589\n",
      "EPOCH: 406, Train Loss: 0.525\n",
      "EPOCH: 407, Train Loss: 0.558\n",
      "EPOCH: 408, Train Loss: 0.576\n",
      "EPOCH: 409, Train Loss: 0.459\n",
      "EPOCH: 410, Train Loss: 0.466\n",
      "EPOCH: 411, Train Loss: 0.425\n",
      "EPOCH: 412, Train Loss: 0.596\n",
      "EPOCH: 413, Train Loss: 0.478\n",
      "EPOCH: 414, Train Loss: 0.485\n",
      "EPOCH: 415, Train Loss: 0.492\n",
      "EPOCH: 416, Train Loss: 0.477\n",
      "EPOCH: 417, Train Loss: 0.527\n",
      "EPOCH: 418, Train Loss: 0.630\n",
      "EPOCH: 419, Train Loss: 0.601\n",
      "EPOCH: 420, Train Loss: 0.516\n",
      "EPOCH: 421, Train Loss: 0.494\n",
      "EPOCH: 422, Train Loss: 0.465\n",
      "EPOCH: 423, Train Loss: 0.442\n",
      "EPOCH: 424, Train Loss: 0.436\n",
      "EPOCH: 425, Train Loss: 0.535\n",
      "EPOCH: 426, Train Loss: 0.463\n",
      "EPOCH: 427, Train Loss: 0.526\n",
      "EPOCH: 428, Train Loss: 0.496\n",
      "EPOCH: 429, Train Loss: 0.414\n",
      "EPOCH: 430, Train Loss: 0.542\n",
      "EPOCH: 431, Train Loss: 0.459\n",
      "EPOCH: 432, Train Loss: 0.517\n",
      "EPOCH: 433, Train Loss: 0.552\n",
      "EPOCH: 434, Train Loss: 0.505\n",
      "EPOCH: 435, Train Loss: 0.466\n",
      "EPOCH: 436, Train Loss: 0.510\n",
      "EPOCH: 437, Train Loss: 0.578\n",
      "EPOCH: 438, Train Loss: 0.512\n",
      "EPOCH: 439, Train Loss: 0.545\n",
      "EPOCH: 440, Train Loss: 0.518\n",
      "EPOCH: 441, Train Loss: 0.416\n",
      "EPOCH: 442, Train Loss: 0.426\n",
      "EPOCH: 443, Train Loss: 0.467\n",
      "EPOCH: 444, Train Loss: 0.471\n",
      "EPOCH: 445, Train Loss: 0.435\n",
      "EPOCH: 446, Train Loss: 0.574\n",
      "EPOCH: 447, Train Loss: 0.528\n",
      "EPOCH: 448, Train Loss: 0.424\n",
      "EPOCH: 449, Train Loss: 0.497\n",
      "EPOCH: 450, Train Loss: 0.449\n",
      "EPOCH: 451, Train Loss: 0.522\n",
      "EPOCH: 452, Train Loss: 0.485\n",
      "EPOCH: 453, Train Loss: 0.468\n",
      "EPOCH: 454, Train Loss: 0.487\n",
      "EPOCH: 455, Train Loss: 0.525\n",
      "EPOCH: 456, Train Loss: 0.534\n",
      "EPOCH: 457, Train Loss: 0.552\n",
      "EPOCH: 458, Train Loss: 0.462\n",
      "EPOCH: 459, Train Loss: 0.525\n",
      "EPOCH: 460, Train Loss: 0.521\n",
      "EPOCH: 461, Train Loss: 0.475\n",
      "EPOCH: 462, Train Loss: 0.539\n",
      "EPOCH: 463, Train Loss: 0.415\n",
      "EPOCH: 464, Train Loss: 0.529\n",
      "EPOCH: 465, Train Loss: 0.530\n",
      "EPOCH: 466, Train Loss: 0.523\n",
      "EPOCH: 467, Train Loss: 0.481\n",
      "EPOCH: 468, Train Loss: 0.482\n",
      "EPOCH: 469, Train Loss: 0.518\n",
      "EPOCH: 470, Train Loss: 0.500\n",
      "EPOCH: 471, Train Loss: 0.581\n",
      "EPOCH: 472, Train Loss: 0.584\n",
      "EPOCH: 473, Train Loss: 0.474\n",
      "EPOCH: 474, Train Loss: 0.490\n",
      "EPOCH: 475, Train Loss: 0.451\n",
      "EPOCH: 476, Train Loss: 0.436\n",
      "EPOCH: 477, Train Loss: 0.523\n",
      "EPOCH: 478, Train Loss: 0.596\n",
      "EPOCH: 479, Train Loss: 0.440\n",
      "EPOCH: 480, Train Loss: 0.474\n",
      "EPOCH: 481, Train Loss: 0.590\n",
      "EPOCH: 482, Train Loss: 0.509\n",
      "EPOCH: 483, Train Loss: 0.457\n",
      "EPOCH: 484, Train Loss: 0.449\n",
      "EPOCH: 485, Train Loss: 0.490\n",
      "EPOCH: 486, Train Loss: 0.484\n",
      "EPOCH: 487, Train Loss: 0.479\n",
      "EPOCH: 488, Train Loss: 0.521\n",
      "EPOCH: 489, Train Loss: 0.560\n",
      "EPOCH: 490, Train Loss: 0.465\n",
      "EPOCH: 491, Train Loss: 0.430\n",
      "EPOCH: 492, Train Loss: 0.503\n",
      "EPOCH: 493, Train Loss: 0.523\n",
      "EPOCH: 494, Train Loss: 0.516\n",
      "EPOCH: 495, Train Loss: 0.482\n",
      "EPOCH: 496, Train Loss: 0.525\n",
      "EPOCH: 497, Train Loss: 0.451\n",
      "EPOCH: 498, Train Loss: 0.505\n",
      "EPOCH: 499, Train Loss: 0.550\n",
      "EPOCH: 500, Train Loss: 0.550\n",
      "EPOCH: 501, Train Loss: 0.464\n",
      "EPOCH: 502, Train Loss: 0.612\n",
      "EPOCH: 503, Train Loss: 0.460\n",
      "EPOCH: 504, Train Loss: 0.435\n",
      "EPOCH: 505, Train Loss: 0.493\n",
      "EPOCH: 506, Train Loss: 0.562\n",
      "EPOCH: 507, Train Loss: 0.537\n",
      "EPOCH: 508, Train Loss: 0.490\n",
      "EPOCH: 509, Train Loss: 0.513\n",
      "EPOCH: 510, Train Loss: 0.510\n",
      "EPOCH: 511, Train Loss: 0.519\n",
      "EPOCH: 512, Train Loss: 0.492\n",
      "EPOCH: 513, Train Loss: 0.456\n",
      "EPOCH: 514, Train Loss: 0.418\n",
      "EPOCH: 515, Train Loss: 0.596\n",
      "EPOCH: 516, Train Loss: 0.477\n",
      "EPOCH: 517, Train Loss: 0.498\n",
      "EPOCH: 518, Train Loss: 0.470\n",
      "EPOCH: 519, Train Loss: 0.552\n",
      "EPOCH: 520, Train Loss: 0.499\n",
      "EPOCH: 521, Train Loss: 0.454\n",
      "EPOCH: 522, Train Loss: 0.577\n",
      "EPOCH: 523, Train Loss: 0.587\n",
      "EPOCH: 524, Train Loss: 0.489\n",
      "EPOCH: 525, Train Loss: 0.543\n",
      "EPOCH: 526, Train Loss: 0.552\n",
      "EPOCH: 527, Train Loss: 0.502\n",
      "EPOCH: 528, Train Loss: 0.514\n",
      "EPOCH: 529, Train Loss: 0.540\n",
      "EPOCH: 530, Train Loss: 0.427\n",
      "EPOCH: 531, Train Loss: 0.432\n",
      "EPOCH: 532, Train Loss: 0.523\n",
      "EPOCH: 533, Train Loss: 0.536\n",
      "EPOCH: 534, Train Loss: 0.463\n",
      "EPOCH: 535, Train Loss: 0.502\n",
      "EPOCH: 536, Train Loss: 0.528\n",
      "EPOCH: 537, Train Loss: 0.562\n",
      "EPOCH: 538, Train Loss: 0.491\n",
      "EPOCH: 539, Train Loss: 0.529\n",
      "EPOCH: 540, Train Loss: 0.566\n",
      "EPOCH: 541, Train Loss: 0.535\n",
      "EPOCH: 542, Train Loss: 0.549\n",
      "EPOCH: 543, Train Loss: 0.534\n",
      "EPOCH: 544, Train Loss: 0.485\n",
      "EPOCH: 545, Train Loss: 0.525\n",
      "EPOCH: 546, Train Loss: 0.435\n",
      "EPOCH: 547, Train Loss: 0.563\n",
      "EPOCH: 548, Train Loss: 0.515\n",
      "EPOCH: 549, Train Loss: 0.386\n",
      "EPOCH: 550, Train Loss: 0.498\n",
      "EPOCH: 551, Train Loss: 0.563\n",
      "EPOCH: 552, Train Loss: 0.594\n",
      "EPOCH: 553, Train Loss: 0.527\n",
      "EPOCH: 554, Train Loss: 0.634\n",
      "EPOCH: 555, Train Loss: 0.490\n",
      "EPOCH: 556, Train Loss: 0.479\n",
      "EPOCH: 557, Train Loss: 0.456\n",
      "EPOCH: 558, Train Loss: 0.423\n",
      "EPOCH: 559, Train Loss: 0.526\n",
      "EPOCH: 560, Train Loss: 0.598\n",
      "EPOCH: 561, Train Loss: 0.441\n",
      "EPOCH: 562, Train Loss: 0.511\n",
      "EPOCH: 563, Train Loss: 0.582\n",
      "EPOCH: 564, Train Loss: 0.491\n",
      "EPOCH: 565, Train Loss: 0.537\n",
      "EPOCH: 566, Train Loss: 0.463\n",
      "EPOCH: 567, Train Loss: 0.469\n",
      "EPOCH: 568, Train Loss: 0.472\n",
      "EPOCH: 569, Train Loss: 0.472\n",
      "EPOCH: 570, Train Loss: 0.454\n",
      "EPOCH: 571, Train Loss: 0.605\n",
      "EPOCH: 572, Train Loss: 0.565\n",
      "EPOCH: 573, Train Loss: 0.492\n",
      "EPOCH: 574, Train Loss: 0.538\n",
      "EPOCH: 575, Train Loss: 0.490\n",
      "EPOCH: 576, Train Loss: 0.521\n",
      "EPOCH: 577, Train Loss: 0.537\n",
      "EPOCH: 578, Train Loss: 0.492\n",
      "EPOCH: 579, Train Loss: 0.495\n",
      "EPOCH: 580, Train Loss: 0.577\n",
      "EPOCH: 581, Train Loss: 0.532\n",
      "EPOCH: 582, Train Loss: 0.602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 583, Train Loss: 0.495\n",
      "EPOCH: 584, Train Loss: 0.483\n",
      "EPOCH: 585, Train Loss: 0.490\n",
      "EPOCH: 586, Train Loss: 0.567\n",
      "EPOCH: 587, Train Loss: 0.467\n",
      "EPOCH: 588, Train Loss: 0.494\n",
      "EPOCH: 589, Train Loss: 0.487\n",
      "EPOCH: 590, Train Loss: 0.482\n",
      "EPOCH: 591, Train Loss: 0.519\n",
      "EPOCH: 592, Train Loss: 0.510\n",
      "EPOCH: 593, Train Loss: 0.539\n",
      "EPOCH: 594, Train Loss: 0.550\n",
      "EPOCH: 595, Train Loss: 0.492\n",
      "EPOCH: 596, Train Loss: 0.525\n",
      "EPOCH: 597, Train Loss: 0.502\n",
      "EPOCH: 598, Train Loss: 0.461\n",
      "EPOCH: 599, Train Loss: 0.441\n",
      "EPOCH: 600, Train Loss: 0.494\n",
      "EPOCH: 601, Train Loss: 0.503\n",
      "EPOCH: 602, Train Loss: 0.537\n",
      "EPOCH: 603, Train Loss: 0.556\n",
      "EPOCH: 604, Train Loss: 0.632\n",
      "EPOCH: 605, Train Loss: 0.618\n",
      "EPOCH: 606, Train Loss: 0.529\n",
      "EPOCH: 607, Train Loss: 0.581\n",
      "EPOCH: 608, Train Loss: 0.512\n",
      "EPOCH: 609, Train Loss: 0.615\n",
      "EPOCH: 610, Train Loss: 0.495\n",
      "EPOCH: 611, Train Loss: 0.560\n",
      "EPOCH: 612, Train Loss: 0.553\n",
      "EPOCH: 613, Train Loss: 0.560\n",
      "EPOCH: 614, Train Loss: 0.494\n",
      "EPOCH: 615, Train Loss: 0.482\n",
      "EPOCH: 616, Train Loss: 0.560\n",
      "EPOCH: 617, Train Loss: 0.531\n",
      "EPOCH: 618, Train Loss: 0.462\n",
      "EPOCH: 619, Train Loss: 0.481\n",
      "EPOCH: 620, Train Loss: 0.489\n",
      "EPOCH: 621, Train Loss: 0.458\n",
      "EPOCH: 622, Train Loss: 0.474\n",
      "EPOCH: 623, Train Loss: 0.576\n",
      "EPOCH: 624, Train Loss: 0.523\n",
      "EPOCH: 625, Train Loss: 0.574\n",
      "EPOCH: 626, Train Loss: 0.458\n",
      "EPOCH: 627, Train Loss: 0.592\n",
      "EPOCH: 628, Train Loss: 0.537\n",
      "EPOCH: 629, Train Loss: 0.512\n",
      "EPOCH: 630, Train Loss: 0.463\n",
      "EPOCH: 631, Train Loss: 0.487\n",
      "EPOCH: 632, Train Loss: 0.567\n",
      "EPOCH: 633, Train Loss: 0.481\n",
      "EPOCH: 634, Train Loss: 0.638\n",
      "EPOCH: 635, Train Loss: 0.537\n",
      "EPOCH: 636, Train Loss: 0.490\n",
      "EPOCH: 637, Train Loss: 0.459\n",
      "EPOCH: 638, Train Loss: 0.544\n",
      "EPOCH: 639, Train Loss: 0.556\n",
      "EPOCH: 640, Train Loss: 0.551\n",
      "EPOCH: 641, Train Loss: 0.434\n",
      "EPOCH: 642, Train Loss: 0.553\n",
      "EPOCH: 643, Train Loss: 0.538\n",
      "EPOCH: 644, Train Loss: 0.588\n",
      "EPOCH: 645, Train Loss: 0.580\n",
      "EPOCH: 646, Train Loss: 0.538\n",
      "EPOCH: 647, Train Loss: 0.551\n",
      "EPOCH: 648, Train Loss: 0.439\n",
      "EPOCH: 649, Train Loss: 0.547\n",
      "EPOCH: 650, Train Loss: 0.505\n",
      "EPOCH: 651, Train Loss: 0.588\n",
      "EPOCH: 652, Train Loss: 0.565\n",
      "EPOCH: 653, Train Loss: 0.462\n",
      "EPOCH: 654, Train Loss: 0.515\n",
      "EPOCH: 655, Train Loss: 0.535\n",
      "EPOCH: 656, Train Loss: 0.549\n",
      "EPOCH: 657, Train Loss: 0.459\n",
      "EPOCH: 658, Train Loss: 0.489\n",
      "EPOCH: 659, Train Loss: 0.458\n",
      "EPOCH: 660, Train Loss: 0.526\n",
      "EPOCH: 661, Train Loss: 0.513\n",
      "EPOCH: 662, Train Loss: 0.471\n",
      "EPOCH: 663, Train Loss: 0.531\n",
      "EPOCH: 664, Train Loss: 0.475\n",
      "EPOCH: 665, Train Loss: 0.527\n",
      "EPOCH: 666, Train Loss: 0.521\n",
      "EPOCH: 667, Train Loss: 0.453\n",
      "EPOCH: 668, Train Loss: 0.580\n",
      "EPOCH: 669, Train Loss: 0.554\n",
      "EPOCH: 670, Train Loss: 0.530\n",
      "EPOCH: 671, Train Loss: 0.522\n",
      "EPOCH: 672, Train Loss: 0.528\n",
      "EPOCH: 673, Train Loss: 0.443\n",
      "EPOCH: 674, Train Loss: 0.436\n",
      "EPOCH: 675, Train Loss: 0.538\n",
      "EPOCH: 676, Train Loss: 0.493\n",
      "EPOCH: 677, Train Loss: 0.640\n",
      "EPOCH: 678, Train Loss: 0.490\n",
      "EPOCH: 679, Train Loss: 0.453\n",
      "EPOCH: 680, Train Loss: 0.472\n",
      "EPOCH: 681, Train Loss: 0.476\n",
      "EPOCH: 682, Train Loss: 0.502\n",
      "EPOCH: 683, Train Loss: 0.474\n",
      "EPOCH: 684, Train Loss: 0.521\n",
      "EPOCH: 685, Train Loss: 0.536\n",
      "EPOCH: 686, Train Loss: 0.488\n",
      "EPOCH: 687, Train Loss: 0.451\n",
      "EPOCH: 688, Train Loss: 0.497\n",
      "EPOCH: 689, Train Loss: 0.602\n",
      "EPOCH: 690, Train Loss: 0.512\n",
      "EPOCH: 691, Train Loss: 0.468\n",
      "EPOCH: 692, Train Loss: 0.524\n",
      "EPOCH: 693, Train Loss: 0.502\n",
      "EPOCH: 694, Train Loss: 0.545\n",
      "EPOCH: 695, Train Loss: 0.461\n",
      "EPOCH: 696, Train Loss: 0.566\n",
      "EPOCH: 697, Train Loss: 0.506\n",
      "EPOCH: 698, Train Loss: 0.499\n",
      "EPOCH: 699, Train Loss: 0.423\n",
      "EPOCH: 700, Train Loss: 0.571\n",
      "EPOCH: 701, Train Loss: 0.533\n",
      "EPOCH: 702, Train Loss: 0.461\n",
      "EPOCH: 703, Train Loss: 0.478\n",
      "EPOCH: 704, Train Loss: 0.510\n",
      "EPOCH: 705, Train Loss: 0.493\n",
      "EPOCH: 706, Train Loss: 0.542\n",
      "EPOCH: 707, Train Loss: 0.514\n",
      "EPOCH: 708, Train Loss: 0.606\n",
      "EPOCH: 709, Train Loss: 0.498\n",
      "EPOCH: 710, Train Loss: 0.549\n",
      "EPOCH: 711, Train Loss: 0.474\n",
      "EPOCH: 712, Train Loss: 0.457\n",
      "EPOCH: 713, Train Loss: 0.625\n",
      "EPOCH: 714, Train Loss: 0.551\n",
      "EPOCH: 715, Train Loss: 0.540\n",
      "EPOCH: 716, Train Loss: 0.418\n",
      "EPOCH: 717, Train Loss: 0.503\n",
      "EPOCH: 718, Train Loss: 0.557\n",
      "EPOCH: 719, Train Loss: 0.541\n",
      "EPOCH: 720, Train Loss: 0.532\n",
      "EPOCH: 721, Train Loss: 0.606\n",
      "EPOCH: 722, Train Loss: 0.437\n",
      "EPOCH: 723, Train Loss: 0.605\n",
      "EPOCH: 724, Train Loss: 0.571\n",
      "EPOCH: 725, Train Loss: 0.553\n",
      "EPOCH: 726, Train Loss: 0.448\n",
      "EPOCH: 727, Train Loss: 0.487\n",
      "EPOCH: 728, Train Loss: 0.459\n",
      "EPOCH: 729, Train Loss: 0.460\n",
      "EPOCH: 730, Train Loss: 0.475\n",
      "EPOCH: 731, Train Loss: 0.560\n",
      "EPOCH: 732, Train Loss: 0.499\n",
      "EPOCH: 733, Train Loss: 0.465\n",
      "EPOCH: 734, Train Loss: 0.487\n",
      "EPOCH: 735, Train Loss: 0.575\n",
      "EPOCH: 736, Train Loss: 0.517\n",
      "EPOCH: 737, Train Loss: 0.470\n",
      "EPOCH: 738, Train Loss: 0.493\n",
      "EPOCH: 739, Train Loss: 0.602\n",
      "EPOCH: 740, Train Loss: 0.433\n",
      "EPOCH: 741, Train Loss: 0.443\n",
      "EPOCH: 742, Train Loss: 0.427\n",
      "EPOCH: 743, Train Loss: 0.580\n",
      "EPOCH: 744, Train Loss: 0.552\n",
      "EPOCH: 745, Train Loss: 0.522\n",
      "EPOCH: 746, Train Loss: 0.474\n",
      "EPOCH: 747, Train Loss: 0.555\n",
      "EPOCH: 748, Train Loss: 0.567\n",
      "EPOCH: 749, Train Loss: 0.492\n",
      "EPOCH: 750, Train Loss: 0.496\n",
      "EPOCH: 751, Train Loss: 0.497\n",
      "EPOCH: 752, Train Loss: 0.487\n",
      "EPOCH: 753, Train Loss: 0.626\n",
      "EPOCH: 754, Train Loss: 0.487\n",
      "EPOCH: 755, Train Loss: 0.412\n",
      "EPOCH: 756, Train Loss: 0.553\n",
      "EPOCH: 757, Train Loss: 0.538\n",
      "EPOCH: 758, Train Loss: 0.500\n",
      "EPOCH: 759, Train Loss: 0.518\n",
      "EPOCH: 760, Train Loss: 0.585\n",
      "EPOCH: 761, Train Loss: 0.511\n",
      "EPOCH: 762, Train Loss: 0.473\n",
      "EPOCH: 763, Train Loss: 0.570\n",
      "EPOCH: 764, Train Loss: 0.527\n",
      "EPOCH: 765, Train Loss: 0.452\n",
      "EPOCH: 766, Train Loss: 0.413\n",
      "EPOCH: 767, Train Loss: 0.512\n",
      "EPOCH: 768, Train Loss: 0.553\n",
      "EPOCH: 769, Train Loss: 0.468\n",
      "EPOCH: 770, Train Loss: 0.611\n",
      "EPOCH: 771, Train Loss: 0.529\n",
      "EPOCH: 772, Train Loss: 0.530\n",
      "EPOCH: 773, Train Loss: 0.526\n",
      "EPOCH: 774, Train Loss: 0.508\n",
      "EPOCH: 775, Train Loss: 0.528\n",
      "EPOCH: 776, Train Loss: 0.602\n",
      "EPOCH: 777, Train Loss: 0.574\n",
      "EPOCH: 778, Train Loss: 0.531\n",
      "EPOCH: 779, Train Loss: 0.512\n",
      "EPOCH: 780, Train Loss: 0.442\n",
      "EPOCH: 781, Train Loss: 0.577\n",
      "EPOCH: 782, Train Loss: 0.501\n",
      "EPOCH: 783, Train Loss: 0.507\n",
      "EPOCH: 784, Train Loss: 0.567\n",
      "EPOCH: 785, Train Loss: 0.617\n",
      "EPOCH: 786, Train Loss: 0.482\n",
      "EPOCH: 787, Train Loss: 0.447\n",
      "EPOCH: 788, Train Loss: 0.456\n",
      "EPOCH: 789, Train Loss: 0.522\n",
      "EPOCH: 790, Train Loss: 0.536\n",
      "EPOCH: 791, Train Loss: 0.466\n",
      "EPOCH: 792, Train Loss: 0.583\n",
      "EPOCH: 793, Train Loss: 0.459\n",
      "EPOCH: 794, Train Loss: 0.487\n",
      "EPOCH: 795, Train Loss: 0.444\n",
      "EPOCH: 796, Train Loss: 0.550\n",
      "EPOCH: 797, Train Loss: 0.536\n",
      "EPOCH: 798, Train Loss: 0.553\n",
      "EPOCH: 799, Train Loss: 0.475\n",
      "EPOCH: 800, Train Loss: 0.537\n",
      "EPOCH: 801, Train Loss: 0.415\n",
      "EPOCH: 802, Train Loss: 0.553\n",
      "EPOCH: 803, Train Loss: 0.475\n",
      "EPOCH: 804, Train Loss: 0.576\n",
      "EPOCH: 805, Train Loss: 0.556\n",
      "EPOCH: 806, Train Loss: 0.477\n",
      "EPOCH: 807, Train Loss: 0.516\n",
      "EPOCH: 808, Train Loss: 0.588\n",
      "EPOCH: 809, Train Loss: 0.500\n",
      "EPOCH: 810, Train Loss: 0.409\n",
      "EPOCH: 811, Train Loss: 0.435\n",
      "EPOCH: 812, Train Loss: 0.491\n",
      "EPOCH: 813, Train Loss: 0.521\n",
      "EPOCH: 814, Train Loss: 0.543\n",
      "EPOCH: 815, Train Loss: 0.508\n",
      "EPOCH: 816, Train Loss: 0.472\n",
      "EPOCH: 817, Train Loss: 0.499\n",
      "EPOCH: 818, Train Loss: 0.555\n",
      "EPOCH: 819, Train Loss: 0.535\n",
      "EPOCH: 820, Train Loss: 0.471\n",
      "EPOCH: 821, Train Loss: 0.626\n",
      "EPOCH: 822, Train Loss: 0.554\n",
      "EPOCH: 823, Train Loss: 0.480\n",
      "EPOCH: 824, Train Loss: 0.561\n",
      "EPOCH: 825, Train Loss: 0.458\n",
      "EPOCH: 826, Train Loss: 0.464\n",
      "EPOCH: 827, Train Loss: 0.521\n",
      "EPOCH: 828, Train Loss: 0.483\n",
      "EPOCH: 829, Train Loss: 0.538\n",
      "EPOCH: 830, Train Loss: 0.444\n",
      "EPOCH: 831, Train Loss: 0.522\n",
      "EPOCH: 832, Train Loss: 0.515\n",
      "EPOCH: 833, Train Loss: 0.526\n",
      "EPOCH: 834, Train Loss: 0.403\n",
      "EPOCH: 835, Train Loss: 0.540\n",
      "EPOCH: 836, Train Loss: 0.485\n",
      "EPOCH: 837, Train Loss: 0.430\n",
      "EPOCH: 838, Train Loss: 0.431\n",
      "EPOCH: 839, Train Loss: 0.536\n",
      "EPOCH: 840, Train Loss: 0.433\n",
      "EPOCH: 841, Train Loss: 0.508\n",
      "EPOCH: 842, Train Loss: 0.415\n",
      "EPOCH: 843, Train Loss: 0.431\n",
      "EPOCH: 844, Train Loss: 0.533\n",
      "EPOCH: 845, Train Loss: 0.422\n",
      "EPOCH: 846, Train Loss: 0.404\n",
      "EPOCH: 847, Train Loss: 0.520\n",
      "EPOCH: 848, Train Loss: 0.494\n",
      "EPOCH: 849, Train Loss: 0.462\n",
      "EPOCH: 850, Train Loss: 0.580\n",
      "EPOCH: 851, Train Loss: 0.498\n",
      "EPOCH: 852, Train Loss: 0.487\n",
      "EPOCH: 853, Train Loss: 0.547\n",
      "EPOCH: 854, Train Loss: 0.541\n",
      "EPOCH: 855, Train Loss: 0.616\n",
      "EPOCH: 856, Train Loss: 0.653\n",
      "EPOCH: 857, Train Loss: 0.497\n",
      "EPOCH: 858, Train Loss: 0.493\n",
      "EPOCH: 859, Train Loss: 0.496\n",
      "EPOCH: 860, Train Loss: 0.504\n",
      "EPOCH: 861, Train Loss: 0.522\n",
      "EPOCH: 862, Train Loss: 0.406\n",
      "EPOCH: 863, Train Loss: 0.516\n",
      "EPOCH: 864, Train Loss: 0.523\n",
      "EPOCH: 865, Train Loss: 0.479\n",
      "EPOCH: 866, Train Loss: 0.448\n",
      "EPOCH: 867, Train Loss: 0.450\n",
      "EPOCH: 868, Train Loss: 0.570\n",
      "EPOCH: 869, Train Loss: 0.568\n",
      "EPOCH: 870, Train Loss: 0.544\n",
      "EPOCH: 871, Train Loss: 0.548\n",
      "EPOCH: 872, Train Loss: 0.489\n",
      "EPOCH: 873, Train Loss: 0.492\n",
      "EPOCH: 874, Train Loss: 0.526\n",
      "EPOCH: 875, Train Loss: 0.476\n",
      "EPOCH: 876, Train Loss: 0.548\n",
      "EPOCH: 877, Train Loss: 0.456\n",
      "EPOCH: 878, Train Loss: 0.454\n",
      "EPOCH: 879, Train Loss: 0.537\n",
      "EPOCH: 880, Train Loss: 0.508\n",
      "EPOCH: 881, Train Loss: 0.529\n",
      "EPOCH: 882, Train Loss: 0.524\n",
      "EPOCH: 883, Train Loss: 0.476\n",
      "EPOCH: 884, Train Loss: 0.548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 885, Train Loss: 0.494\n",
      "EPOCH: 886, Train Loss: 0.540\n",
      "EPOCH: 887, Train Loss: 0.554\n",
      "EPOCH: 888, Train Loss: 0.525\n",
      "EPOCH: 889, Train Loss: 0.482\n",
      "EPOCH: 890, Train Loss: 0.476\n",
      "EPOCH: 891, Train Loss: 0.543\n",
      "EPOCH: 892, Train Loss: 0.443\n",
      "EPOCH: 893, Train Loss: 0.433\n",
      "EPOCH: 894, Train Loss: 0.511\n",
      "EPOCH: 895, Train Loss: 0.554\n",
      "EPOCH: 896, Train Loss: 0.517\n",
      "EPOCH: 897, Train Loss: 0.518\n",
      "EPOCH: 898, Train Loss: 0.507\n",
      "EPOCH: 899, Train Loss: 0.552\n",
      "EPOCH: 900, Train Loss: 0.429\n",
      "EPOCH: 901, Train Loss: 0.475\n",
      "EPOCH: 902, Train Loss: 0.483\n",
      "EPOCH: 903, Train Loss: 0.457\n",
      "EPOCH: 904, Train Loss: 0.544\n",
      "EPOCH: 905, Train Loss: 0.594\n",
      "EPOCH: 906, Train Loss: 0.503\n",
      "EPOCH: 907, Train Loss: 0.495\n",
      "EPOCH: 908, Train Loss: 0.531\n",
      "EPOCH: 909, Train Loss: 0.431\n",
      "EPOCH: 910, Train Loss: 0.523\n",
      "EPOCH: 911, Train Loss: 0.606\n",
      "EPOCH: 912, Train Loss: 0.533\n",
      "EPOCH: 913, Train Loss: 0.539\n",
      "EPOCH: 914, Train Loss: 0.495\n",
      "EPOCH: 915, Train Loss: 0.506\n",
      "EPOCH: 916, Train Loss: 0.513\n",
      "EPOCH: 917, Train Loss: 0.446\n",
      "EPOCH: 918, Train Loss: 0.517\n",
      "EPOCH: 919, Train Loss: 0.531\n",
      "EPOCH: 920, Train Loss: 0.424\n",
      "EPOCH: 921, Train Loss: 0.514\n",
      "EPOCH: 922, Train Loss: 0.528\n",
      "EPOCH: 923, Train Loss: 0.544\n",
      "EPOCH: 924, Train Loss: 0.491\n",
      "EPOCH: 925, Train Loss: 0.478\n",
      "EPOCH: 926, Train Loss: 0.595\n",
      "EPOCH: 927, Train Loss: 0.469\n",
      "EPOCH: 928, Train Loss: 0.386\n",
      "EPOCH: 929, Train Loss: 0.454\n",
      "EPOCH: 930, Train Loss: 0.486\n",
      "EPOCH: 931, Train Loss: 0.565\n",
      "EPOCH: 932, Train Loss: 0.628\n",
      "EPOCH: 933, Train Loss: 0.556\n",
      "EPOCH: 934, Train Loss: 0.520\n",
      "EPOCH: 935, Train Loss: 0.452\n",
      "EPOCH: 936, Train Loss: 0.562\n",
      "EPOCH: 937, Train Loss: 0.482\n",
      "EPOCH: 938, Train Loss: 0.564\n",
      "EPOCH: 939, Train Loss: 0.546\n",
      "EPOCH: 940, Train Loss: 0.510\n",
      "EPOCH: 941, Train Loss: 0.527\n",
      "EPOCH: 942, Train Loss: 0.528\n",
      "EPOCH: 943, Train Loss: 0.496\n",
      "EPOCH: 944, Train Loss: 0.527\n",
      "EPOCH: 945, Train Loss: 0.492\n",
      "EPOCH: 946, Train Loss: 0.569\n",
      "EPOCH: 947, Train Loss: 0.512\n",
      "EPOCH: 948, Train Loss: 0.467\n",
      "EPOCH: 949, Train Loss: 0.476\n",
      "EPOCH: 950, Train Loss: 0.446\n",
      "EPOCH: 951, Train Loss: 0.481\n",
      "EPOCH: 952, Train Loss: 0.487\n",
      "EPOCH: 953, Train Loss: 0.589\n",
      "EPOCH: 954, Train Loss: 0.537\n",
      "EPOCH: 955, Train Loss: 0.469\n",
      "EPOCH: 956, Train Loss: 0.431\n",
      "EPOCH: 957, Train Loss: 0.483\n",
      "EPOCH: 958, Train Loss: 0.470\n",
      "EPOCH: 959, Train Loss: 0.475\n",
      "EPOCH: 960, Train Loss: 0.498\n",
      "EPOCH: 961, Train Loss: 0.540\n",
      "EPOCH: 962, Train Loss: 0.413\n",
      "EPOCH: 963, Train Loss: 0.494\n",
      "EPOCH: 964, Train Loss: 0.469\n",
      "EPOCH: 965, Train Loss: 0.530\n",
      "EPOCH: 966, Train Loss: 0.570\n",
      "EPOCH: 967, Train Loss: 0.540\n",
      "EPOCH: 968, Train Loss: 0.491\n",
      "EPOCH: 969, Train Loss: 0.484\n",
      "EPOCH: 970, Train Loss: 0.567\n",
      "EPOCH: 971, Train Loss: 0.460\n",
      "EPOCH: 972, Train Loss: 0.468\n",
      "EPOCH: 973, Train Loss: 0.397\n",
      "EPOCH: 974, Train Loss: 0.443\n",
      "EPOCH: 975, Train Loss: 0.449\n",
      "EPOCH: 976, Train Loss: 0.523\n",
      "EPOCH: 977, Train Loss: 0.528\n",
      "EPOCH: 978, Train Loss: 0.471\n",
      "EPOCH: 979, Train Loss: 0.514\n",
      "EPOCH: 980, Train Loss: 0.572\n",
      "EPOCH: 981, Train Loss: 0.503\n",
      "EPOCH: 982, Train Loss: 0.473\n",
      "EPOCH: 983, Train Loss: 0.602\n",
      "EPOCH: 984, Train Loss: 0.582\n",
      "EPOCH: 985, Train Loss: 0.460\n",
      "EPOCH: 986, Train Loss: 0.590\n",
      "EPOCH: 987, Train Loss: 0.508\n",
      "EPOCH: 988, Train Loss: 0.520\n",
      "EPOCH: 989, Train Loss: 0.563\n",
      "EPOCH: 990, Train Loss: 0.518\n",
      "EPOCH: 991, Train Loss: 0.511\n",
      "EPOCH: 992, Train Loss: 0.442\n",
      "EPOCH: 993, Train Loss: 0.458\n",
      "EPOCH: 994, Train Loss: 0.439\n",
      "EPOCH: 995, Train Loss: 0.554\n",
      "EPOCH: 996, Train Loss: 0.507\n",
      "EPOCH: 997, Train Loss: 0.576\n",
      "EPOCH: 998, Train Loss: 0.553\n",
      "EPOCH: 999, Train Loss: 0.503\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "for epoch in range(n_epochs):\n",
    "    loss_train = train(actor_model, device, dataloader_train, loss_function, optimizer)\n",
    "    scheduler.step()\n",
    "    print('EPOCH: {}, Train Loss: {:.3f}'.format(epoch, loss_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32b047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e60e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to ./models/maholo_SAC_actor_weights.pth\n"
     ]
    }
   ],
   "source": [
    "savepath = \"./models/maholo_SAC_actor_weights.pth\"\n",
    "torch.save(actor_model.state_dict(), savepath)\n",
    "print(\"Saved to\", savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c938f0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088d246",
   "metadata": {},
   "source": [
    "# Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a4e0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = suite.make(\n",
    "    env_name=\"Lift\",\n",
    "    robots=\"Panda\",\n",
    "    has_renderer=True,\n",
    "    has_offscreen_renderer=True,\n",
    "    use_camera_obs=False,\n",
    "    control_freq=50,\n",
    "    horizon = 50,\n",
    ")\n",
    "env = GymWrapper(env)\n",
    "env = TimeFeatureWrapper(env)\n",
    "model = DDPG.load(modelpath, env = env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff41f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic = True)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    env.unwrapped.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d01bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
